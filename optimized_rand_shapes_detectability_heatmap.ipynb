{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea9b721-4790-45b6-943f-19e96eae7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "from EightBitTransit.cTransitingImage import TransitingImage\n",
    "from EightBitTransit.inversion import *\n",
    "from EightBitTransit.misc import *\n",
    "from scipy.optimize import lsq_linear\n",
    "import tqdm as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Wedge\n",
    "\n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "# Add spocc directory to the path to import spocc modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import data\n",
    "import loaders\n",
    "\n",
    "\n",
    "# These \"magic\" functions work in jupyter notebooks, this one reloads\n",
    "# modules so if you make changes, you don't have to restart the notebook.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from extern import features\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from multiprocessing import Pool\n",
    "sys.path.append('..')\n",
    "import loaders\n",
    "import data\n",
    "from extern.quarterTools import data_scaler\n",
    "\n",
    "import warnings\n",
    "from lightkurve import LightkurveWarning\n",
    "warnings.filterwarnings(\"ignore\", category=LightkurveWarning)\n",
    "\n",
    "from matplotlib.path import Path\n",
    "\n",
    "\n",
    "SEED = 12345\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04085069-1a54-452e-b6b0-195c8d836175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inject squares\n",
    "#pick one lightcurve\n",
    "#width/depth plot\n",
    "#detected if rank < 80k and M > 0.25\n",
    "#start with mag 14-15 bin\n",
    "\n",
    "#right now change by hand shape, d/w, tic id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6113a2b-aae0-419c-a7ad-a9069c3b9f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#round shapes\n",
    "\n",
    "\n",
    "def make_shape(image_ratio, velocity, shape, seed, vertices, t_ref):\n",
    "    data_dir = './Random_LCs'\n",
    "    image_ratio = image_ratio\n",
    "    image_ratio_high = 1 + .5* (image_ratio -1)\n",
    "    image_ratio_low = -.5 * (image_ratio-1)\n",
    "    #print(image_ratio_high, image_ratio_low)\n",
    "    \n",
    "    v = float(velocity)\n",
    "\n",
    "    if shape == 'Circle':\n",
    "        circle = plt.Circle((0.5, 0.5), 0.5, color='black')\n",
    "\n",
    "        fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot\n",
    "        # (or if you have an existing figure)\n",
    "        # fig = plt.gcf()\n",
    "        # ax = fig.gca()\n",
    "\n",
    "        ax.add_patch(circle)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(image_ratio_low,image_ratio_high)\n",
    "        \n",
    "        os.makedirs(data_dir + \"/Class_%s\" %(shape), exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s.png\" %(t_ref)\n",
    "        fig.savefig(img_file_name ,bbox_inches='tight', pad_inches = 0)\n",
    "\n",
    "    if shape == 'Triangle':\n",
    "        pts = np.array([[0,0], [1,0], [.5,0.866025]],)\n",
    "        p = Polygon(pts, closed=False, fc = 'black')\n",
    "        ax = plt.gca()\n",
    "        ax.add_patch(p)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.axis('off')\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(image_ratio_low,image_ratio_high)\n",
    "\n",
    "        os.makedirs(data_dir + \"/Class_%s\" %(shape), exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s.png\" %(t_ref)\n",
    "        plt.savefig(img_file_name ,bbox_inches='tight', pad_inches = 0)\n",
    "    \n",
    "    if shape == 'Square':\n",
    "        plt.axes()\n",
    "        square = plt.Rectangle((0,0),1,1, fc='black',ec=\"black\")\n",
    "        plt.gca().add_patch(square)\n",
    "        plt.axis('scaled')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ylim(image_ratio_low,image_ratio_high)\n",
    "        plt.xlim(0,1)\n",
    "        #plt.savefig(\"square.png\",bbox_inches='tight', pad_inches = 0)\n",
    "\n",
    "        #display plot\n",
    "        #plt.gcf()\n",
    "        \n",
    "        os.makedirs(data_dir + \"/Class_%s\" %(shape), exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s.png\" %(t_ref)\n",
    "        plt.savefig(img_file_name ,bbox_inches='tight', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "    if shape == 'Random':\n",
    "        np.random.seed(seed)\n",
    "\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        n = vertices # Number of possibly sharp edges\n",
    "        r = .9# magnitude of the perturbation from the unit circle, \n",
    "        # should be between 0 and 1\n",
    "        N = n*3+1 # number of points in the Path\n",
    "        # There is the initial point and 3 points per cubic bezier curve. Thus, the curve will only pass though n points, which will be the sharp edges, the other 2 modify the shape of the bezier curve\n",
    "\n",
    "        angles = np.linspace(0,2*np.pi,N)\n",
    "        codes = np.full(N,Path.CURVE4)\n",
    "        codes[0] = Path.MOVETO\n",
    "\n",
    "        verts = np.stack((np.cos(angles),np.sin(angles))).T*(.5*r*np.random.random(N)+1-r)[:,None]\n",
    "        verts = verts + np.array([.5, .5])\n",
    "        verts[-1,:] = verts[0,:] # Using this instad of Path.CLOSEPOLY avoids an innecessary straight line\n",
    "        path = Path(verts, codes)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        patch = patches.PathPatch(path, facecolor='black', lw=1)\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "        ax.set_xlim(np.min(verts)*1, np.max(verts)*1)\n",
    "        ax.set_ylim(np.min(verts)*1, np.max(verts)*1)\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        #to plot vertices\n",
    "        #x, y = zip(*path.vertices)\n",
    "        #line, = ax.plot(x, y, 'go-')\n",
    "        plt.axis('off')\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(image_ratio_low,image_ratio_high)\n",
    "        #plt.plot()\n",
    "        \n",
    "        os.makedirs(data_dir + \"/Class_%s\" %(shape), exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s\" %(t_ref) +\"_\"+ str(seed) + \"_\" + str(vertices) +\".png\"\n",
    "        plt.savefig(img_file_name ,bbox_inches='tight', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    if shape == 'Donut':\n",
    "        outer_circle = plt.Circle((0.5, 0.5), 0.5, color='black')\n",
    "        inner_circle = plt.Circle((0.25, 0.5), 0.2, facecolor='white', edgecolor='white')\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.add_patch(outer_circle)\n",
    "        ax.add_patch(inner_circle)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(image_ratio_low, image_ratio_high)\n",
    "        plt.show()\n",
    "\n",
    "        os.makedirs(data_dir + \"/Class_%s\" % shape, exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s.png\" %(t_ref)\n",
    "\n",
    "        fig.savefig(img_file_name, bbox_inches='tight', pad_inches=0, transparent =True)\n",
    "        \n",
    "    if shape == 'Panels':\n",
    "        plt.axes()\n",
    "        square = plt.Rectangle((0,0),6,1, fc='black',ec=\"black\")\n",
    "        inner_circle = plt.Circle((3, .25), .25, facecolor='white', edgecolor='white')\n",
    "\n",
    "        plt.gca().add_patch(square)\n",
    "        plt.gca().add_patch(inner_circle)\n",
    "\n",
    "        plt.axis('scaled')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ylim(image_ratio_low,image_ratio_high)\n",
    "        plt.xlim(0,6)\n",
    "\n",
    "\n",
    "        #display plot\n",
    "        plt.gcf()\n",
    "        \n",
    "        os.makedirs(data_dir + \"/Class_%s\" %(shape), exist_ok=True)\n",
    "        img_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s.png\" %(t_ref)\n",
    "        plt.savefig(img_file_name ,bbox_inches='tight', pad_inches = 0)\n",
    "\n",
    "    #######\n",
    "    \n",
    "    # Read back in the image file.\n",
    "\n",
    "    # an array of times, in days, over which to calculate the light curve \n",
    "    times = np.linspace(0,27,1296)\n",
    "\n",
    "    file = img_file_name\n",
    "\n",
    "    lowres_SItitle = TransitingImage(imfile=file,\n",
    "                                     lowres=32,\n",
    "                                     lowrestype=\"mean\", # Calculate the lower-resolution version of the image by averaging \"neighborhoods\" of pixels in the high-res image.\n",
    "                                     lowresround=False, # Let the resulting low-res pixel values take on intermediate values between 0 and 1, i.e. don't round them to 0 or 1.\n",
    "                                     v=velocity,\n",
    "                                     t_ref=t_ref,\n",
    "                                     t_arr=times,\n",
    "                                     LDlaw = \"linear\",\n",
    "                                     LDCs = [0.4]\n",
    "                                    )\n",
    "    ##plot lowres version of image\n",
    "    #lowres_SItitle.plot_grid()\n",
    "    \n",
    "    lowres_SItitle_LC, overlapTimes = lowres_SItitle.gen_LC(t_arr=times) \n",
    "    \n",
    "    #plot lightcurve\n",
    "    #fig, ax = plt.subplots(1,1,figsize=(16,4))\n",
    "    #ax.plot(overlapTimes,lowres_SItitle_LC,color=\"#1969ea\",ls=\"-\",lw=5)\n",
    "    #ax.set_xlim(-10,10)\n",
    "    #plt.xlabel(\"Time [days]\",fontsize=14)\n",
    "    #plt.ylabel(\"Relative flux\",fontsize=14)\n",
    "    #plt.title(r\"The low-res image's light curve as it transits left-to-right across the star at $v = 0.3 d^{-1}$\",fontsize=16)\n",
    "    #plt.show()\n",
    "    \n",
    "    #print(\"min: \" + str(min(lowres_SItitle_LC)))\n",
    "\n",
    "    \n",
    "    list_of_tuples = list(zip(overlapTimes, lowres_SItitle_LC))\n",
    "    \n",
    "    txt_file_name = data_dir + \"/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity) + \"_t_%s\" %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices)\n",
    "    \n",
    "    df = pd.DataFrame(list_of_tuples, columns=['Time', 'Flux'])\n",
    "    df.to_csv(txt_file_name + '.csv', index=False)\n",
    "    \n",
    "    #np.savetxt(txt_file_name, lowres_SItitle_LC, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68822934-83c7-4fc9-a102-158a4065494a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " #how do we want to add LCs to TESS curves?\n",
    "import math\n",
    "\n",
    "def make_lc(shape, image_ratio, velocity, seed, vertices, tic_id, t_ref):\n",
    "    #shape = \"Triangle\"\n",
    "    #ratio = \"2\"\n",
    "    #index=3\n",
    "    index = filtered_subref[filtered_subref['TIC_ID'] == int(tic_id)].index.values\n",
    "    #print(index)\n",
    "    f0 = data_dir+filtered_subref.loc[index[0], \"Filename\"]    \n",
    "    tic_id = f0.split('_')[-1].split('.')[0] #get the tic_id from the string\n",
    "    #print(f' tic2 {tic_id}')\n",
    "    lc = loaders.load_lc(f0)\n",
    "    #print(lc)\n",
    "    lc = lc[lc.quality==0]\n",
    "    lc = lc.normalize()\n",
    "    flux_err = lc.flux_err\n",
    "    #print(f0)\n",
    "    #print(flux_err[0])\n",
    "    #print(lc.time.value)\n",
    "    time = lc.time.value\n",
    "    time = [value - time[0] for value in time] #make time start at 0\n",
    "    # plt.scatter(time, lc.flux.value)\n",
    "    # plt.title(\"TESS Flux Only\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    noise = pd.read_csv('/home/jupyter/SPOcc/spocc/notebooks/Random_LCs/Class_%s' %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity)+'_t_%s' %(t_ref)+ \"_\"  + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "\n",
    "    sim_time = noise.iloc[:,0].values\n",
    "    sim_flux = noise.iloc[:,1].values\n",
    "    sim_flux = [x - 1 for x in sim_flux]\n",
    "\n",
    "\n",
    "    #print(sim_time)\n",
    "    # plt.scatter(sim_time, sim_flux)\n",
    "    # plt.title(\"Flux From 8BT\")\n",
    "    # plt.show()\n",
    "    #print(sim_time[-1])\n",
    "\n",
    "    after_interval = math.ceil((27 - sim_time[-1]) / 0.0208333333) #days to 30 minutes\n",
    "\n",
    "    sim_time_after = np.linspace(math.ceil(sim_time[-1]), 27, after_interval)\n",
    "    #print(sim_time_after)\n",
    "\n",
    "    sim_time = np.append(sim_time, sim_time_after)\n",
    "\n",
    "\n",
    "    sim_flux_after = np.zeros(len(sim_time_after))\n",
    "\n",
    "    sim_flux = np.append(sim_flux,sim_flux_after)\n",
    "\n",
    "    before_interval = math.ceil((sim_time[0]) / 0.0208333333) #days to 30 minutes\n",
    "\n",
    "    sim_time_before = np.linspace(0, math.floor(sim_time[0]), before_interval)\n",
    "    #print(sim_time_before)\n",
    "\n",
    "    sim_time = np.insert(sim_time, 0, sim_time_before)\n",
    "\n",
    "\n",
    "    sim_flux_before = np.zeros(len(sim_time_before))\n",
    "\n",
    "    sim_flux = np.insert(sim_flux, 0, sim_flux_before)    \n",
    "\n",
    "\n",
    "\n",
    "#     plt.scatter(sim_time, sim_flux)\n",
    "#     plt.title(\"Extended 8BT Flux\")\n",
    "#     plt.ylabel(\"Relative flux\",fontsize=14)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "    sim_flux = np.interp(time, sim_time, sim_flux) \n",
    "    #print(sim_flux)\n",
    "\n",
    "    combined_flux = lc.flux.value + sim_flux\n",
    "\n",
    "#     plt.scatter(time, sim_flux)\n",
    "#     plt.title(\"Interpolated 8BT Flux\")\n",
    "#     plt.show()\n",
    "    #plt.title(f\"Signal Injected into TIC {tic_id}\")\n",
    "    #plt.plot(time, combined_flux)\n",
    "    #plt.ylabel('Normalized FLux', fontsize=14)\n",
    "    #plt.xlabel(\"Time [days]\",fontsize=14)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    #os.makedirs('./Injected_LCs_tic' +tic_id, exist_ok=True)\n",
    "    \n",
    "    directory_path = os.path.join('.', 'Injected_LCs/tic' + tic_id)\n",
    "    # Attempt to create the directory with proper error handling\n",
    "    try:\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "        #print(\"Directory created successfully!\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating the directory: {e}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print('./Injected_LCs_tic' + str(int(tic_id)))\n",
    "\n",
    "    list_of_tuples = list(zip(time, combined_flux, flux_err))\n",
    "\n",
    "    txt_file_name = './Injected_LCs/tic' + tic_id + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity)+'_t_%s' %(t_ref)+ \"_\" + str(seed) + \"_\" + str(vertices)\n",
    "\n",
    "    df = pd.DataFrame(list_of_tuples, columns=['Time', 'Flux', 'Flux_err'])\n",
    "    df.to_csv(txt_file_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394a6fe9-581b-47b2-a858-7ed2951a7a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_mstat(tic_id, shape, image_ratio, velocity, seed, vertices, t_ref):\n",
    "\n",
    "    \n",
    "    df = pd.read_csv('/home/jupyter/SPOcc/spocc/notebooks/Injected_LCs/tic' + str(int(tic_id)) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity)+'_t_%s' %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "    #print(df)\n",
    "    flux = df['Flux'].values\n",
    "    \n",
    "    avg = np.nanmedian(flux)\n",
    "    # print('avg ' + str(avg))\n",
    "    stdev = np.nanstd(flux)\n",
    "    # print('stdev ' + str(stdev))\n",
    "\n",
    "    orderflux = np.sort(flux)\n",
    "    num = len(orderflux)\n",
    "    # Get top and bottom deciles of the lightcurve data:\n",
    "    extremeup = orderflux[0:int(np.round(0.1*num))]\n",
    "    #print(len(extremeup), extremeup)\n",
    "    extremedown = orderflux[int(np.round(0.9*num)):num-1]\n",
    "    #print(len(extremedown), extremedown)    \n",
    "    extreme = np.append(extremeup,extremedown)\n",
    "    #print(len(extreme), extreme)\n",
    "    \n",
    "    #print(avg, np.mean(extreme), stdev)\n",
    "    diff = (avg-np.mean(extreme))/stdev\n",
    "    return diff\n",
    "\n",
    "def find_width_depth(shape, ratio, velocity, seed, vertices, t_ref):\n",
    "\n",
    "    df = pd.read_csv('/home/jupyter/SPOcc/spocc/notebooks/Random_LCs/Class_' + str(shape) + '/' +str(shape) + '_ratio_'+str(ratio)+\"_velocity_%s\" %(velocity)+'_t_%s' %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "    \n",
    "    flux = df['Flux'].values\n",
    "    time = df['Time'].values\n",
    "\n",
    "\n",
    "    #plt.plot(time, flux,label='velocity' +str(velocity), color = 'grey')\n",
    "\n",
    "\n",
    "    nonzero_indices = df.index[flux != 1]\n",
    "    first_nonzero_index = nonzero_indices[0]\n",
    "    last_nonzero_index = nonzero_indices[-1]\n",
    "    width = df.loc[last_nonzero_index, 'Time'] - df.loc[first_nonzero_index, 'Time']\n",
    "    depth = np.min(flux)\n",
    "\n",
    "    return width, depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b30807-c886-49ed-8b06-e427db40ff9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_subset(s):\n",
    "    \"\"\"Returns the subset features used to rescore a sector.\n",
    "    \n",
    "    Note: only Sector 18 has been rescored and had the reference subset\n",
    "    saved as of 7/7/2023. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = feats  # if feats is defined in an earlier block\n",
    "    except:\n",
    "        datafile = \"/home/jupyter/mountpoint/dataproducts/db.v2.h5\"\n",
    "        df = pd.read_hdf(datafile, f\"S{s}/features\")\n",
    "\n",
    "    subset_tics = np.loadtxt(f\"/home/jupyter/mountpoint/dataproducts/s{s}_score_reference.txt\")\n",
    "    sample_subset = df.loc[subset_tics]\n",
    "    return sample_subset\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def data_scaler(data_to_scale, s=18):\n",
    "    \"\"\"Scales given features by the same factors as the given sector.\n",
    "    \n",
    "    This method recalculates the factors needed to scale each feature for a\n",
    "    given sector. The features are scaled and shifted such that each set of\n",
    "    features has a mean of zero and a standard deviation of one for\n",
    "    the features calculated for all light curves in the given sector.\n",
    "    \n",
    "    Though possible, newly scaled data should not be expected to have a mean\n",
    "    of zero or a standard a deviation of one for any feature.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = feats.iloc[:, 0:61]\n",
    "    except: \n",
    "        datafile = \"/home/jupyter/mountpoint/dataproducts/db.v2.h5\"\n",
    "        df = pd.read_hdf(datafile, f\"S{s}/features\")[:, 0:61]  # including mstat but not astat\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(df)\n",
    "    scaled = scaler.transform(data_to_scale)\n",
    "    scaled = pd.DataFrame(index=data_to_scale.index,\n",
    "                                columns=df.columns,\n",
    "                                data=scaled)\n",
    "    return scaled\n",
    "\n",
    "def score_scaler(score):\n",
    "    \"\"\"Rescales \"raw\" scores to match Sector 18 rescored\n",
    "    \n",
    "    Calculated based on reference points. Not perfect, but pretty good\n",
    "    Note: can't do similar to data_scaler because \"raw\" scores aren't saved\n",
    "    \"\"\"\n",
    "    factor = 0.00018255470902209855\n",
    "    offset = -4.34047914797675e-12\n",
    "    scaled = score*factor+offset\n",
    "    return scaled\n",
    "\n",
    "def dist_scores(ref_data, d2s=None, k=1, scaler=False):\n",
    "    \"\"\"\n",
    "    This method calculates the distance to the k-th neighbor in the reference data\n",
    "    Args:\n",
    "        ref_data (Numpy array or Pandas dataframe) - The reference data to which\n",
    "            distances will be calculated\n",
    "        d2s (Numpy array or Pandas dataframe) - Data to be scored (if None,\n",
    "            same as the reference data)\n",
    "        k (integer) - neighbor to calculate the distance\n",
    "\n",
    "    Returns:\n",
    "        scores (Numpy array) - distances for data in d2s.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if isinstance(d2s, type(None)):\n",
    "        # if d2s is a dataframe, the evaluation of d2s==None tries to compare\n",
    "        # every value of the dataframe to None, instead we check if it's\n",
    "        # NoneType\n",
    "        d2s = ref_data  # will default to full data if d2s is not specified\n",
    "\n",
    "    if isinstance(scaler, type(data_scaler)):\n",
    "        ref_data = scaler(ref_data)\n",
    "        d2s = scaler(d2s)\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=k+1, algorithm='ball_tree', n_jobs=-1).fit(ref_data)\n",
    "    distances, indices = nbrs.kneighbors(d2s)\n",
    "\n",
    "    scores = score_scaler(distances[:, k])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f824d160-03ec-443d-bbd3-1e34581aeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Dan Comment\n",
    "Put all your global variables up at the top, everything that only\n",
    "needs to be defined once\n",
    "\"\"\"\n",
    "\n",
    "#def get_shape_fluxes(shape):\n",
    "#    fluxes = pd.read_csv('./Injected_LCs/tic' + str(int(tic_id)) + \"/%s\" %(shape)  +\"_ratio_%s\" %(image_ratio) +\"_velocity_%s\" %(velocity)+'_t_%s' %(t_ref)  + \"_\" + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "#    return fluxes\n",
    "\n",
    "def get_test_features(shape):\n",
    "    \"\"\"Dan Comment:\n",
    "    This function will import the injected light curves and get the features\n",
    "    for them.\n",
    "\n",
    "    These features won't change even if the scaling later does, this is a time\n",
    "    consuming function so you want to redo it as little as possible, only the\n",
    "    once if you can. I'd recommend just saving the features as variables for\n",
    "    the notebook.\n",
    "    \"\"\"\n",
    "    to_score_total = pd.DataFrame()\n",
    "\n",
    "    #fluxes = get_shape_fluxes(shape)\n",
    "    \n",
    "    for w in widths:\n",
    "        for d in depths:\n",
    "            for t_ref in t_refs:\n",
    "                len_obj = 1/d\n",
    "                v = 2*(1 + (len_obj)) / w\n",
    "                #d = d.round(decimals=2)\n",
    "                #v = v.round(decimals=2)\n",
    "\n",
    "                # Dan: changed the range since there are different numbers of columns\n",
    "                try:\n",
    "                    #print('./Injected_LCs/tic' + str(int(tic_id)) + \"/%s\" %(shape)  +\"_ratio_%s\" %(d) +\"_velocity_%s\" %(v)+'_t_%s.csv' %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "                    fluxes = pd.read_csv('./Injected_LCs/tic' + str(int(tic_id)) + \"/%s\" %(shape)  +\"_ratio_%s\" %(d) +\"_velocity_%s\" %(v)+'_t_%s' %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices) +\".csv\")\n",
    "                    test_flux = fluxes[\"Flux\"].values\n",
    "                    test_err = np.zeros_like(test_flux, dtype='uint8')\n",
    "                    test_time = fluxes[\"Time\"].values\n",
    "                except:\n",
    "                    # Dan: this will stop the loop once it hits an unrecognized column\n",
    "                    break\n",
    "                test_features = features.feats(test_time, test_flux, test_err)\n",
    "\n",
    "                test_features = pd.DataFrame(columns=test_features.keys(), data=[test_features.values()])\n",
    "\n",
    "                mstat = features.calc_mstat(test_flux)\n",
    "\n",
    "                test_features['mstat'] = mstat\n",
    "                to_score_total = to_score_total.append(test_features)\n",
    "    #print(to_score_total)\n",
    "    return to_score_total\n",
    "\n",
    "\n",
    "#print(feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656daa40-09c2-4c85-b3ca-9ac6bca88248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Dan Comment\n",
    "There's way too much output to do all three shapes at the same time.\n",
    "Split it up so it's legible.\n",
    "\n",
    "Since it was a loop, might as well make it a function\n",
    "\n",
    "and I prefer to separate functionality from plotting,\n",
    "plotting can be slow and functions can need to be changed often\n",
    "\"\"\"\n",
    "def function(shape):\n",
    "    \"\"\"Dan Comment\n",
    "    I removed the placeholder arrays, opting instead to define things more\n",
    "    directly in the lines they get calculated in\n",
    "    \"\"\"\n",
    "    to_score = feats_dict[shape]\n",
    "    #print(to_score)\n",
    "    # Do the scoring all at once\n",
    "    # Got rid of the plotting here, can plot later\n",
    "    scores = dist_scores(sample_subset, to_score, k=1, scaler=data_scaler)\n",
    "    scores_df = pd.DataFrame(data=scores.T, columns=[\"scores\"])\n",
    "    \n",
    "    # instead of a for loop around everything, I only implement a couple\n",
    "    # in making specific lists\n",
    "    shape_score = [sector_scores[sector_scores.scores>scores[i]]['rank'].max()+1\n",
    "                   for i in range(len(to_score))]\n",
    "    scores_df['ranks'] = shape_score \n",
    "    #fluxes = get_shape_fluxes(shape)\n",
    "    #scores_df['min_flux'] = [min(fluxes[\"Flux%s\" %(i+1)].values[:-200])\n",
    "    #                      for i in range(len(to_score))]\n",
    "    #scores_df['mstat'] = fluxes['Mstats']\n",
    "    #scores_df['widths'] = fluxes['Widths']\n",
    "    \n",
    "    # I opt to organize things in dataframes because it keeps related information\n",
    "    # tied together\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536840c1-b721-4256-89d3-6517be34dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount data either from disk or bucket\n",
    "\n",
    "mag_bin = 6\n",
    "\n",
    "data_dir = \"/home/jupyter/mountpoint/\"  # path for bucket\n",
    "\n",
    "ref = loaders.load_ref(18, data_dir) #sector 18 camera 2\n",
    "\n",
    "feats = pd.read_hdf(\"/home/jupyter/mountpoint/dataproducts/db.v2.h5\", \"S18/features\")\n",
    "sector_scores = pd.read_hdf(\"/home/jupyter/mountpoint/dataproducts/s18rescore.h5\", \"S18/scores\")\n",
    "sample_subset = get_subset(18).iloc[:,:61]\n",
    "\n",
    "\n",
    "#subref = ref[ref.TIC_ID.isin([251630511])]#377199128 # overplot with this anomalous\n",
    "subref = ref.loc[(ref['Magnitude']>mag_bin) & (ref['Magnitude']<mag_bin+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a55b00-5e30-4bf4-8da2-cb1b6dff399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Ranks:  721413.0  -  2164216.0\n",
      "Range of Mstats:  -1.226  -  1.282\n"
     ]
    }
   ],
   "source": [
    "#print(sector_scores)\n",
    "#print(feats)\n",
    "#sample_subset\n",
    "new_df = feats.reset_index()\n",
    "#print(new_df)\n",
    "merged = sector_scores.merge(new_df, left_on='TIC_ID', right_on='index')[['mstat', 'TIC_ID', 'rank', 'scores']]\n",
    "#print(merged)\n",
    "\n",
    "q25 = merged['rank'].quantile(0.25)\n",
    "q75 = merged['rank'].quantile(0.75)\n",
    "\n",
    "# Filter the DataFrame to include only the middle 50% of ranks\n",
    "merged_filtered = merged[(merged['rank'] >= q25) & (merged['rank'] <= q75)]\n",
    "\n",
    "median_mstat = merged['mstat'].median()\n",
    "sigma = merged['mstat'].std()\n",
    "\n",
    "# Calculate the lower and upper threshold for dropping rows\n",
    "lower_threshold = median_mstat - sigma\n",
    "upper_threshold = median_mstat + sigma\n",
    "\n",
    "# Drop rows where mstat is within 1 sigma of the median\n",
    "merged_filtered = merged_filtered[(merged_filtered['mstat'] < lower_threshold) | (merged_filtered['mstat'] > upper_threshold)]\n",
    "#print(merged_filtered)\n",
    "\n",
    "#print(\"Range of Ranks before: \", merged['rank'].min(), \" - \", merged['rank'].max())\n",
    "#print(\"Range of Mstats before: \", merged['mstat'].min(), \" - \", merged['mstat'].max())\n",
    "\n",
    "print(\"Range of Ranks: \", merged_filtered['rank'].min(), \" - \", merged_filtered['rank'].max())\n",
    "print(\"Range of Mstats: \", merged_filtered['mstat'].min(), \" - \", merged_filtered['mstat'].max())\n",
    "\n",
    "#print(merged_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36070d66-c1d7-4416-b67b-a76b320ffdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAG BIN: 6\n"
     ]
    }
   ],
   "source": [
    "#merged_filtered = merged_filtered.head(10)\n",
    "tic_ids = np.array([])\n",
    "filtered_subref = subref.merge(merged_filtered, left_on='TIC_ID', right_on='TIC_ID')\n",
    "#print(filtered_subref)\n",
    "\n",
    "\n",
    "\n",
    "lcc = loaders.LightCurveCollection(filtered_subref)\n",
    "print(f'MAG BIN: {mag_bin}')\n",
    "for i in range(0,10):\n",
    "    tic_id = str(filtered_subref.TIC_ID[i])\n",
    "    tic_ids = np.append(tic_ids, tic_id)\n",
    "    lc = lcc.load_cut_lc(lcc[i])\n",
    "    time = lc.time.value\n",
    "    time = [value - time[0] +2.8 for value in time] #make time start at 0\n",
    "    #plt.plot(time, lc.flux.value)\n",
    "    #plt.title(f\"TIC {tic_id}\")\n",
    "    #plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ed4334-2259-4eb3-9c5c-45e030571fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_data(args):\n",
    "    w, d, t_ref , tic_id, seed, vertices = args\n",
    "    len_obj = 1/d\n",
    "    v = 2*(1 + (len_obj)) / w\n",
    "    #print(f' tic1 {tic_id}')\n",
    "    #v = v.round(decimals=2)\n",
    "    #d = d.round(decimals=2)\n",
    "    rand_file_path = \"./Random_LCs/Class_%s\" %(shape) + \"/%s\" %(shape)  +\"_ratio_%s\" %(d) +\"_velocity_%s\" %(v) + \"_t_%s\" %(t_ref) + \"_\" + str(seed) + \"_\" + str(vertices) + \".csv\"\n",
    "    if  os.path.exists(rand_file_path):\n",
    "        #print(\"Shape already generated\")\n",
    "        pass\n",
    "    else:\n",
    "        make_shape(d, v, shape, seed, vertices, t_ref)\n",
    "    width, depth = find_width_depth(shape, d, v, seed, vertices, t_ref)\n",
    "    inj_file_path = './Injected_LCs/tic' + tic_id + \"/%s\" %(shape)  +\"_ratio_%s\" %(d) +\"_velocity_%s\" %(v)+'_t_%s' %(t_ref)+ \"_\" + str(seed) + \"_\" + str(vertices) + \".csv\"\n",
    "    \n",
    "    if  os.path.exists(inj_file_path):\n",
    "        #print(\"Lightcurve already generated\")\n",
    "        pass\n",
    "    else:\n",
    "        make_lc(shape, d, v, seed, vertices, tic_id, t_ref)\n",
    "    mstat = calc_mstat(tic_id, shape, d, v, seed, vertices, t_ref)\n",
    "    \n",
    "    len_obj = 1/d\n",
    "    return width, depth, t_ref, mstat, len_obj\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(seed, vertices, tic_id):\n",
    "    global widths, depths, t_refs, feats_dict, all_results\n",
    "\n",
    "    if shape == 'Triangle':\n",
    "        depths = np.array([0.07, 0.16, 0.25, 0.34, 0.43, .49])\n",
    "        depths = 1 / np.sqrt(1 - 2 * depths)\n",
    "    else:\n",
    "        depths = np.array([0.07, 0.16, 0.25, 0.34, 0.43, 0.52, 0.61, 0.70, 0.79, 0.88, 0.97, .99])\n",
    "        depths = 1 / np.sqrt(1 - depths)\n",
    "    widths = np.array([.3, .4, .5, .7, 1.0, 1.8])\n",
    "    widths = np.array([1.,2.,3.,4.,5.,6.])\n",
    "    #widths = np.array([7,8,9,10,11])\n",
    "\n",
    "    #how do I change the velocities so that the widths are the same every time? it'll depend on depth\n",
    "    t_refs = np.array(range(8, 18))\n",
    "\n",
    "    results = []\n",
    "    len_objs = np.array([])\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for w in widths:\n",
    "            for d in depths:\n",
    "\n",
    "                args_list = [(w, d, t_ref, tic_id, seed, vertices) for t_ref in t_refs]\n",
    "\n",
    "                results.extend(pool.map(process_data, args_list)) #figure out what's going on in here\n",
    "        #print(len_objs)\n",
    "    results_widths = np.array([r[0] for r in results])\n",
    "    results_depths = np.array([r[1] for r in results])\n",
    "    results_t_refs = np.array([r[2] for r in results])\n",
    "    mstats = np.array([r[3] for r in results])\n",
    "    len_objs = np.array([r[4] for r in results])\n",
    "\n",
    "    feats_dict = {shape:get_test_features(shape)}\n",
    "\n",
    "    results_rankings = function(shape).ranks.values\n",
    "\n",
    "    scores = pd.DataFrame({'Mstats': mstats, 'Rankings': results_rankings})\n",
    "    scores[\"Detected\"] = ((scores[\"Mstats\"] > 0.25) & (scores[\"Rankings\"] < 80000)).astype(int)\n",
    "\n",
    "    averaged_detected = np.mean(scores[\"Detected\"].values.reshape(-1, len(t_refs)), axis=1)\n",
    "    n_results_widths = results_widths[::len(t_refs)]\n",
    "    n_results_depths = results_depths[::len(t_refs)]\n",
    "\n",
    "    import seaborn as sns\n",
    "    #print(f'w len: {len(n_results_widths)}, d len: {len(n_results_depths)}, detected len: {len(averaged_detected)}')\n",
    "    n_results_depths = n_results_depths.round(decimals=2)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame({'Width': n_results_widths, 'Flux at mid transit': n_results_depths, 'Detected' : averaged_detected})], ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    #data_pivoted = data.pivot(\"Width\", \"Flux at mid transit\", \"Detected\")\n",
    "\n",
    "    #data_pivoted = sns.heatmap(data_pivoted, cmap=\"viridis\")\n",
    "\n",
    "    #print(data_pivoted)\n",
    "\n",
    "    #plt.title(f\"TIC {tic_id}\")\n",
    "    #data_pivoted.collections[0].colorbar.set_label(\"Detected %\")\n",
    "    #data_pivoted.invert_yaxis()\n",
    "    #plt.show()\n",
    "\n",
    "    #os.makedirs('./rand_heatmap_data_mag_' + str(mag_bin),exist_ok=True)\n",
    "    #data.to_csv('./rand_heatmap_data_mag_' + str(mag_bin) + '/' + str(int(tic_id)) + '_seed_' + str(seed) + \"_vert_\" + str(vertices) +'_heatmap.csv', index=False)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ce2c6-db6a-4a4d-a069-ee6de2fb4108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['379603165' '252137537' '317646251' '286636275' '308873470' '322437808'\n",
      " '468450664' '420287575' '410232577' '306554613']\n",
      "MAG BIN: 6\n"
     ]
    }
   ],
   "source": [
    "shape = \"Random\"\n",
    "\n",
    "all_results = pd.DataFrame(columns=['Width', 'Flux at mid transit', 'Detected'])\n",
    "\n",
    "print(tic_ids)\n",
    "print(f'MAG BIN: {mag_bin}')\n",
    "\n",
    "vertices = [10]\n",
    "seeds = np.arange(0,25)\n",
    "\n",
    "for vertices in vertices:\n",
    "    for seed in seeds:\n",
    "        for tic_id in tic_ids:\n",
    "            pipeline(seed, vertices, tic_id)\n",
    "   \n",
    "os.makedirs('./rand_heatmap_data_mag_' + str(mag_bin),exist_ok=True)\n",
    "csv_file_path = './rand_heatmap_data_mag_' + str(mag_bin) + '/accumulated_results.csv'\n",
    "all_results.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795ffc9-158e-4683-8fe6-ad18fb332f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8df4f3d-f540-4b95-bddf-8c97e2d2c4d1",
   "metadata": {},
   "source": [
    "#Below is the \"chunkified\" version of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbb824-1f64-40ac-9a17-4c3852ca4d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "shape = 'Random'\n",
    "tic_id = '357686299'\n",
    "seed = 0\n",
    "vertices = 5\n",
    "\n",
    "\n",
    "global widths, depths, t_refs, feats_dict\n",
    "\n",
    "if shape == 'Triangle':\n",
    "    depths = np.array([0.07, 0.16, 0.25, 0.34, 0.43, .49])\n",
    "    depths = 1 / np.sqrt(1 - 2 * depths)\n",
    "else:\n",
    "    depths = np.array([0.07, 0.16, 0.25, 0.34, 0.43, 0.52, 0.61, 0.70, 0.79, 0.88, 0.97, .99])\n",
    "    depths = 1 / np.sqrt(1 - depths)\n",
    "widths = np.array([.3, .4, .5, .7, 1.0, 1.8])\n",
    "widths = np.array([1.,2.,3.,4.,5.,6.])\n",
    "#how do I change the velocities so that the widths are the same every time? it'll depend on depth\n",
    "t_refs = np.array(range(8, 18))\n",
    "\n",
    "results = []\n",
    "len_objs = np.array([])\n",
    "with multiprocessing.Pool() as pool:\n",
    "    for w in widths:\n",
    "        for d in depths:\n",
    "\n",
    "            args_list = [(w, d, t_ref, seed, vertices, tic_id) for t_ref in t_refs]\n",
    "\n",
    "            results.extend(pool.map(process_data, args_list)) #figure out what's going on in here\n",
    "    #print(len_objs)\n",
    "results_widths = np.array([r[0] for r in results])\n",
    "results_depths = np.array([r[1] for r in results])\n",
    "results_t_refs = np.array([r[2] for r in results])\n",
    "mstats = np.array([r[3] for r in results])\n",
    "len_objs = np.array([r[4] for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3ba3a-7d4e-469f-b390-0e88c37ad898",
   "metadata": {},
   "outputs": [],
   "source": [
    "mstats = np.array([])\n",
    "for w in widths:\n",
    "    for d in depths:        \n",
    "        for t_ref in t_refs:\n",
    "            len_obj = 1/d\n",
    "            #v = v.round(decimals=2)\n",
    "            #d=d.round(decimals=2\n",
    "\n",
    "            v = 2*(1 + (len_obj)) / w\n",
    "            mstat = calc_mstat(tic_id, shape, d, v, t_ref)\n",
    "            mstats = np.append(mstats, mstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28c38d-f2e4-4439-90b4-5973aa405948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats_dict = {#\"Circle\":get_test_features('Circle'), \n",
    "              \"Square\":get_test_features('Square'), \n",
    "              #\"Triangle\":get_test_features('Triangle')\n",
    "                }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ff436-41ed-4a50-8f72-02e117d617f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50321cdd-8e41-4117-9ddc-eb07583006a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rankings = function(shape).ranks.values\n",
    "print(results_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac8506-491e-457d-84ed-88432a3affca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_rankings))\n",
    "print(len(mstats))\n",
    "scores = pd.DataFrame({'Mstats': mstats, 'Rankings': results_rankings})\n",
    "scores[\"Detected\"] = ((scores[\"Mstats\"] > 0.25) & (scores[\"Rankings\"] < 80000)).astype(int)\n",
    "print(scores)\n",
    "averaged_detected = np.mean(scores[\"Detected\"].values.reshape(-1, len(t_refs)), axis=1)\n",
    "print(len(averaged_detected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ae9d7-7c95-435f-9f2a-a14c67b0b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results_widths = results_widths[::10]\n",
    "n_results_depths = results_depths[::10]\n",
    "print(len(n_results_widths), len(averaged_detected), len(scores['Mstats']))\n",
    "\n",
    "mstats_avg = np.mean(scores[\"Mstats\"].values.reshape(-1, len(t_refs)), axis=1)\n",
    "print(len(mstats_avg))\n",
    "rankings_avg = np.mean(scores[\"Rankings\"].values.reshape(-1, len(t_refs)), axis=1)\n",
    "print(len(rankings_avg))\n",
    "data = pd.DataFrame({'Width': n_results_widths, 'Event Depth': n_results_depths, 'Mstats' : mstats_avg})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b9060-3666-4ab2-a1d3-097dd76b754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "n_results_depths = n_results_depths.round(decimals=2)\n",
    "data = pd.DataFrame({'Width': n_results_widths, 'Event Depth': n_results_depths, 'Mstats' : mstats_avg})\n",
    "#data = pd.DataFrame({'Width': n_results_widths, 'Event Depth': n_results_depths, 'Rankings' : rankings_avg})\n",
    "\n",
    "data['Event Depth'] = data['Event Depth'].replace(0.02, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "data_pivoted = data.pivot(\"Width\", \"Event Depth\", \"Mstats\")\n",
    "#data_pivoted = data.pivot(\"Width\", \"Event Depth\", \"Rankings\")\n",
    "\n",
    "data_pivoted = sns.heatmap(data_pivoted, cmap=\"viridis\")\n",
    "#pd.plot(kind='RonR',rot=0)\n",
    "\n",
    "\n",
    "\n",
    "#plt.title(str(noise) + \" noise transit detection rate\")\n",
    "data_pivoted.collections[0].colorbar.set_label(\"Mstats\")\n",
    "#data_pivoted.collections[0].colorbar.set_label(\"Rankings\")\n",
    "\n",
    "data_pivoted.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# os.makedirs('./heatmap_data_mag_' + str(mag_bin),exist_ok=True)\n",
    "# data.to_csv('./heatmap_data_mag_' + str(mag_bin) + '/' + tic_id + '_heatmap.csv', index=False)#took 10 minutes\n",
    "\n",
    "\n",
    "\n",
    "#for index, row in data.iterrows():\n",
    "#    plt.plot(data.columns, row, label=f\"Width {index}\")\n",
    "\n",
    "# Add legend\n",
    "#plt.legend()\n",
    "\n",
    "#plt.show()\n",
    "plt.scatter( data['Width'],data['Mstats'],)\n",
    "plt.xlabel('Widths')\n",
    "plt.ylabel('Mstats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea919da-7dda-48ad-b41c-03e915715759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start 12:54\n",
    "from time import gmtime, strftime\n",
    "print (strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7333a0-9282-42df-99f8-2af0a4b6c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note depth variance vs targeted depth\n",
    "#change csv names to reflect width & depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373909fd-1596-4b91-9fb6-a422f3add717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_subset = get_subset(18)\n",
    "squares = get_test_features('Square')\n",
    "circles = get_test_features('Circle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58409d-508f-4f47-b971-56ad7b09d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#num_columns = features.shape[1]\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 11\n",
    "\n",
    "fig, axs = plt.subplots(31, 2, figsize=(30,120))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, column in enumerate(squares.columns):\n",
    "    axs[i].hist(sample_subset[column], bins=100, label = 'sector 18 sample')\n",
    "    axs[i].hist(squares[column], bins=100, color = 'red')\n",
    "    axs[i].hist(circles[column],bins=100, color ='black')\n",
    "    axs[i].set_title(f\"{column} Histogram\")\n",
    "    \n",
    "        \n",
    "    for injected_value in squares[column]:\n",
    "        axs[i].axvline(injected_value, color='red', linestyle='dashed', label = 'Squares')\n",
    "    for value in circles[column]:\n",
    "        axs[i].axvline(value, color='black', linestyle='dashed', label = \"Circles\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e548e3-f9c9-4f80-a676-d504f652e2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "spocc",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "spocc",
   "language": "python",
   "name": "spocc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
